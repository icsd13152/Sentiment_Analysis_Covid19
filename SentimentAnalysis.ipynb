{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08729a88",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Covid-19 Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ac6bb",
   "metadata": {},
   "source": [
    "# # Abstract\n",
    "ÎšÎ±Ï„Î¬ Ï„Î·Î½ Î´Î¹Î¬ÏÎºÎµÎ¹Î± Ï„Î·Ï‚ Ï€Î±Î½Î´Î·Î¼Î¯Î±Ï‚, Ï€Î¿Î»Î»Î¿Î¯ Ï‡ÏÎ®ÏƒÏ„ÎµÏ‚ Ï„Î¿Ï… Twitter Î”Î·Î¼Î¿ÏƒÎ¹ÎµÏÎ¿Ï…Î½ Î´Î¹Î¬Ï†Î¿ÏÎ± Posts Ï€Î¿Ï… Î±Ï€Î¿Ï„ÎµÎ»Î¿ÏÎ½ Ï„Î·Î½ Î³Î½ÏÎ¼Î· Ï„Î¿Ï… ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¬ Î¼Îµ Ï„Î¿Î½ Covid-19. Î˜Î± Î®Ï„Î±Î½ ÎºÎ±Î»ÏŒ Î½Î± Î³Î½Ï‰ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ ÏƒÏ…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î± Ï€Î¿Ï… Ï€ÏÎ¿ÎºÏÏ€Ï„ÎµÎ¹ Î±Ï€ÏŒ Ï„Î¿ ÎºÎ¬Î¸Îµ post, Ï€ÏÎ¿ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Î½Î± Î´Î¿ÏÎ¼Îµ ÎºÎ±Ï„Î¬ Ï€ÏŒÏƒÎ¿ Î­Ï‡ÎµÎ¹ ÎµÏ€Î·ÏÎµÎ±ÏƒÏ„ÎµÎ¯ Î· ÏˆÏ…Ï‡Î¿Î»Î¿Î³Î¯Î± Ï„Î¿Ï… ÎµÎºÎ¬ÏƒÏ„Î¿Ï„Îµ Ï‡ÏÎ®ÏƒÏ„Î·."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca1665",
   "metadata": {},
   "source": [
    "Î¤Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®Î¸Î·ÎºÎ±Î½ ÎºÎ±Ï„Î± Ï„Î¹Ï‚ training/validation Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯ÎµÏ‚ Î¼Ï€Î¿ÏÎµÎ¯ ÎºÎ±Î½ÎµÎ¯Ï‚ Î½Î± Ï„Î± Î²ÏÎµÎ¯ ÏƒÏ„Î¿ Kaggle ÏƒÏ„Î¿ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ URL:\n",
    "https://www.kaggle.com/gauravsahani/covid-19-sentiment-analysis-using-spacy/data\n",
    "\n",
    "ÎœÏ€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ±Ï„ÎµÎ²Î¬ÏƒÎ¿Ï…Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Îµ Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ command:\n",
    "\n",
    "$ kaggle kernels output gauravsahani/covid-19-sentiment-analysis-using-spacy -p /path/to/dest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa784d0",
   "metadata": {},
   "source": [
    "# # Data Processing\n",
    "Î Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î±Ï†Î¿Ï Î­Ï‡Î¿Ï…Î¼Îµ ÎºÎ±Ï„ÎµÎ²Î¬ÏƒÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€Î¿ Ï„Î¿ Kaggle, Î´Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î¿ csv file Ï„Î¿ Î¿Ï€Î¿Î¯Î¿ Î±Ï†Î¿ÏÎ¬ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï„Î¿Ï… Training.\n",
    "ÎŒÏ€Ï‰Ï‚ Î¸Î± Ï€Î±ÏÎ±Ï„Î·ÏÎ®ÏƒÎ¿Ï…Î¼Îµ ÏƒÏ„Î¿ Kaggle Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Î±Ï‚ ÎµÎ¯Î½Î±Î¹ Ï‡Ï‰ÏÎ¹ÏƒÎ¼Î­Î½Î± ÏƒÎµ 2 csv Î±ÏÏ‡ÎµÎ¯Î±. ÎˆÎ½Î± Train ÎºÎ±Î¹ Î­Î½Î± Test.\n",
    "ÎˆÏ„ÏƒÎ¹ Î»Î¿Î¹Ï€ÏŒÎ½ Î±Ï†Î¿Ï ÎºÎ¬Î½Î¿Ï…Î¼Îµ import Ï„Î¹Ï‚ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„ÎµÏ‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ Ï€Î¿Ï… Î¸Î± Ï‡ÏÎµÎ¹Î±ÏƒÏ„Î¿ÏÎ¼Îµ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Î´Î¹Î±Î²Î¬ÏƒÎ¿Ï…Î¼Îµ Ï„Î± csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2cfb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserName  ScreenName   Location     TweetAt  \\\n",
      "0      3799       48751     London  16-03-2020   \n",
      "1      3800       48752         UK  16-03-2020   \n",
      "2      3801       48753  Vagabonds  16-03-2020   \n",
      "3      3802       48754        NaN  16-03-2020   \n",
      "4      3803       48755        NaN  16-03-2020   \n",
      "\n",
      "                                       OriginalTweet           Sentiment  \n",
      "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
      "1  advice Talk to your neighbours family to excha...            Positive  \n",
      "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
      "3  My food stock is not the only one which is emp...            Positive  \n",
      "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  \n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\n",
    "import nltk\n",
    "from nltk.corpus import  wordnet\n",
    "# nltk.download('wordnet')\n",
    "from nltk import FreqDist, PorterStemmer, SnowballStemmer, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn import feature_selection\n",
    "\n",
    "data = pd.read_csv(r\"Corona_NLP_train.csv\", encoding='ansi')\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e8d259",
   "metadata": {},
   "source": [
    "Î‘Ï†Î¿Ï Î´Î¹Î±Î²Î¬ÏƒÎ±Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€Î¿ Ï„Î¿ CSV Î±ÏÏ‡ÎµÎ¯Î¿ ÎºÎ±Î¹ Ï„Î± Ï†Î­ÏÎ±Î¼Îµ ÏƒÎµ Î­Î½Î± DataFrame Î±ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯ Î· ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¹Î± Î±Ï…Ï„ÏÎ½ ÎºÎ±Î¹ Î· Î´Î¹ÎµÏÎµÏÎ½Î·ÏƒÎ· Î³Î¹Î± Ï„Î¿ Ï„Î¹ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î­Ï‡Î¿Ï…Î¼Îµ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b8005d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"Sentiment\"] = data[\"Sentiment\"].replace('Extremely Negative', 'Negative', regex=True)\n",
    "\n",
    "data[\"Sentiment\"] = data[\"Sentiment\"].replace('Extremely Positive', 'Positive', regex=True)\n",
    "\n",
    "#transform Sentiment to number\n",
    "#negative=0\n",
    "#postive=1\n",
    "#neutral=2\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Negative', -1, regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Positive', 1, regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Neutral', 0, regex=True)\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(data[\"Sentiment\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79db5f6",
   "metadata": {},
   "source": [
    "ÎŒÏ€Ï‰Ï‚ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Î´Î¿ÏÎ¼Îµ Î³Î¹Î± Ï„Î·Î½ Class = Negative = -1 Î­Ï‡Î¿Ï…Î¼Îµ Ï€ÎµÏÎ¯Ï€Î¿Ï… 15.500 samples. ÎŸÎ¼Î¿Î¯Ï‰Ï‚ Î³Î¹Î± Ï„Î· Class = Positive = 1 Î­Ï‡Î¿Ï…Î¼Îµ Ï€ÎµÏÎ¯Ï€Î¿Ï… 17.500 samples. Î¤Î­Î»Î¿Ï‚ Î³Î¹Î± Ï„Î·Î½ Class = Neutral = 0 Î­Ï‡Î¿Ï…Î¼Îµ Ï€ÎµÏÎ¯Ï€Î¿Ï… 7.500 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0b246",
   "metadata": {},
   "source": [
    "## Î‘ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯ Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Ï„Î¿Ï… Processing:\n",
    "ÎœÎµÏ„Î¬ Î±Ï€Î¿ Ï€Î¿Î»Ï ÏˆÎ¬Î¾Î¹Î¼Î¿ ÎµÎ½Ï„ÏŒÏ‚ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï€Î±ÏÎ±Ï„Î®ÏÎ·ÏƒÎ± Î¿Ï„Î¹ Î­Ï‡Î¿Ï…Î¼Îµ ÎºÎ¬Ï€Î¿Î¹ÎµÏ‚ Ï€ÎµÏÎ¯ÎµÏÎ³ÎµÏ‚ Î»Î­Î¾ÎµÎ¹Ï‚ ÎºÎ±Î¹ ÎºÎ¬Ï€Î¿Î¹ÎµÏ‚ Î»Î­Î¾ÎµÎ¹Ï‚ Î¼Îµ Î±Ï€ÏŒÏƒÏ„ÏÎ¿Ï†Î¿. \n",
    "Î•Ï€Î¯ÏƒÎ·Ï‚ ÎµÎ¯Î½Î±Î¹ Î»Î¿Î³Î¹ÎºÏŒ ÏƒÎµ Î­Î½Î± tweet Î½Î± Î²ÏÎµÎ¹ ÎºÎ±Î½ÎµÎ¯Ï‚ ÎºÎ±Î¹ emojis/emoticons. ÎŒÎ»Î± Î±Ï…Ï„Î¬ Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ ÏƒÎµ ÎºÎ¬Ï€Î¿Î¹Î± dictionariesÎ¼Îµ ÏƒÎºÎ¿Ï€ÏŒ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ lookups\n",
    "Î³Î¹Î± Î½Î± Î¼Ï€ÏÎ¿ÏÎ¼Îµ Î½Î± Ï„Î± Î±Î½Ï„Î¹ÎºÎ±Î¸Î¹ÏƒÏ„Î¿ÏÎ¼Îµ Î® Î½Î± Ï„Î± Î´Î¹Î±Î³ÏÎ¬Ï†Î¿Ï…Î¼Îµ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae492c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#short words/apostrophe lookup\n",
    "contraction_dict1 = {\"Ã‚\":\"\",\"â€™\":\"'\",\"Ãƒ\":\"\"}\n",
    "contraction_dict2 = {\"Ã‚\":\"\",\"â€™\":\"'\",\"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\"don't\": \"do not\",\"Don't\":\"Do not\",\n",
    "                     \"I'll\":\"I will\",\"Didn't\":\"Did not\",\"hasn't\":\"has not\",\"NYC\":\"New York City\",\"16MAR20\":\"\",\n",
    "                     \"I'd\":\"I would\",\"I've\":\"I have\",\"you're\":\"you are\",\"I'm\":\"I am\",\"it's\":\"it is\",\n",
    "                     \"#NZ\":\"\",\"they'll\":\"they will\",\"they're\":\"they are\",\"can't\":\"can not\",\"Y'all\":\"You All\",\n",
    "                     \"I m\":\"I am\",\"can't\":\"can not\",\"don t\":\"do not\",\"I ve\":\"I have\",\"we're\":\"we are\",\n",
    "                     \"LOL\":\"lough out loud\",\"lol\":\"lough out loud\",\"FYI\":\"For your information\",\"OFC\":\"Of Course\",\"ofc\":\"Of Course\",\n",
    "                     \"#coronavirÃƒÂ¼s\":\"coronavirus\",\"pls\":\"please\",\"#stayhomesavelives\":\"stay home save lives\",\n",
    "                     \"hasn't\": 'has not',\"haven't\": 'have not',\"he'd\": 'he had / he would',\"he'd've\": 'he would have',\n",
    "                     \"he'll\": 'he shall / he will',\"he'll've\": 'he shall have / he will have',\n",
    "                     \"he's\": 'he has / he is',\"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will',\n",
    "                     \"how's\": 'how has / how is', \"i'd\": 'I had / I would',\"i'd've\": 'I would have',  \"i'll\": 'I shall / I will',\n",
    "                     \"i'll've\": 'I shall have / I will have',\"i'm\": 'I am', \"i've\": 'I have', \"isn't\": 'is not', \"it'd\": 'it had / it would',\n",
    "                     \"it'd've\": 'it would have', \"it'll\": 'it shall / it will',\n",
    "                     \"it'll've\": 'it shall have / it will have',\"it's\": 'it has / it is', \"let's\": 'let us',\n",
    "                     \"ma'am\": 'madam', \"mayn't\": 'may not',\n",
    "                     \"might've\": 'might have', \"mightn't\": 'might not',\n",
    "                     \"mightn't've\": 'might not have', \"must've\": 'must have',\"mustn't\": 'must not',\n",
    "                     \"mustn't've\": 'must not have', \"needn't\": 'need not',\n",
    "                     \"needn't've\": 'need not have', \"o'clock\": 'of the clock',\n",
    "                     \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have',\n",
    "                     \"shan't\": 'shall not', \"sha'n't\": 'shall not',\n",
    "                     \"shan't've\": 'shall not have', \"she'd\": 'she had / she would',\n",
    "                     \"she'd've\": 'she would have', \"she'll\": 'she shall / she will',\n",
    "                     \"she'll've\": 'she shall have / she will have',\n",
    "                     \"she's\": 'she has / she is', \"should've\": 'should have',\n",
    "                     \"shouldn't\": 'should not',\"shouldn't've\": 'should not have',\n",
    "                     \"so've\": 'so have', \"so's\": 'so as / so is',\n",
    "                     \"that'd\": 'that would / that had',\"that'd've\": 'that would have',\n",
    "                     \"that's\": 'that has / that is', \"there'd\": 'there had / there would',\n",
    "                     \"there'd've\": 'there would have', \"there's\": 'there has / there is',\n",
    "                     \"they'd\": 'they had / they would',  \"they'd've\": 'they would have',\n",
    "                     \"they'll\": 'they shall / they will', \"they'll've\": 'they shall have / they will have',\n",
    "                     \"they're\": 'they are',  \"they've\": 'they have',\n",
    "                     \"to've\": 'to have', \"wasn't\": 'was not',\n",
    "                     \"we'd\": 'we had / we would',  \"we'd've\": 'we would have',\n",
    "                     \"we'll\": 'we will', \"we'll've\": 'we will have',\n",
    "                     \"we're\": 'we are', \"we've\": 'we have',\n",
    "                     \"weren't\": 'were not', \"what'll\": 'what shall / what will',\n",
    "                     \"what'll've\": 'what shall have / what will have',\n",
    "                     \"what're\": 'what are', \"what's\": 'what has / what is',\n",
    "                     \"what've\": 'what have',\"when's\": 'when has / when is',\n",
    "                     \"when've\": 'when have', \"where'd\": 'where did',\n",
    "                     \"where's\": 'where has / where is',\n",
    "                     \"where've\": 'where have', \"who'll\": 'who shall / who will',\n",
    "                     \"who'll've\": 'who shall have / who will have',\n",
    "                     \"who's\": 'who has / who is', \"who've\": 'who have',\n",
    "                     \"why's\": 'why has / why is', \"why've\": 'why have',\n",
    "                     \"will've\": 'will have', \"won't\": 'will not',\"won't've\": 'will not have',\n",
    "                     \"would've\": 'would have',\"wouldn't\": 'would not',\"wouldn't've\": 'would not have',\n",
    "                     \"y'all\": 'you all', \"y'all'd\": 'you all would',\n",
    "                     \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are',\n",
    "                     \"y'all've\": 'you all have', \"you'd\": 'you had / you would',\n",
    "                     \"you'd've\": 'you would have',\"&amp\":\"and\",\"btc\":\"bitcoin\",\"irs\":\"\",\"spx\":\"\",\"ğŸ“\":\"\",\"âœ…\":\"\"\n",
    "                     }\n",
    "\n",
    "emoticons={':)': 'happy', ':â€‘)': 'happy',\n",
    " ':-]': 'happy', ':-3': 'happy',\n",
    " ':->': 'happy', '8-)': 'happy',\n",
    " ':-}': 'happy', ':o)': 'happy',\n",
    " ':c)': 'happy', ':^)': 'happy',\n",
    " '=]': 'happy', '=)': 'happy',\n",
    " '<3': 'happy', ':-(': 'sad',\n",
    " ':(': 'sad', ':c': 'sad',\n",
    " ':<': 'sad', ':[': 'sad',\n",
    " '>:[': 'sad', ':{': 'sad',\n",
    " '>:(': 'sad', ':-c': 'sad',\n",
    " ':-< ': 'sad', ':-[': 'sad',\n",
    " ':-||': 'sad',\n",
    "  'ğŸ˜¢':'sad'         }\n",
    "\n",
    "myOwnStopWords={'price':\"\",\n",
    "               'store':\"\",\n",
    "               'supermarket':\"\",\n",
    "               'food':\"\",\n",
    "               'grocery':\"\",\n",
    "               'people':\"\",\n",
    "               'go':\"\",\n",
    "               'consumer':\"\",\n",
    "                'usdjpy':\"\", 'gbpusd':\"\", 'usdcnh':\"\", 'xauusd':\"\", 'wti':\"\", 'spx':\"\",'iave':\"\",\"aiave\":\"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87682d9d",
   "metadata": {},
   "source": [
    "Î£ÎºÎ¿Ï€ÏŒÏ‚ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î½Î± ÏƒÎ²Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î± emojis/emoticos Î±Î»Î»Î¬ Î½Î± Ï„Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ replace Î¼Îµ Ï„Î·Î½ Î»Î­Î¾Î· Ï€Î¿Ï… Î´ÎµÎ¯Ï‡Î½Î¿Ï…Î½. ÎœÎµ Î±Ï…Ï„ÏŒÎ½ Ï„Î¿Î½ Ï„ÏÏŒÏ€Î¿ Î¸Î± Î¼Ï€ÏÎ¿Î¿ÏÎ¼Îµ Î½Î± ÎºÏÎ±Ï„Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ ÏƒÏ…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î± Ï€Î¿Ï… Ï€ÏÎ¿Î²Î¬Î»Î»Î¿Ï…Î½. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9cacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_dict(text, dictionary):\n",
    "    if isinstance(text, float) == False and text is not None:\n",
    "        for word in text.split():\n",
    "            if word.lower() in dictionary:\n",
    "                if word.lower() in text.split():\n",
    "                    text = text.replace(word, dictionary[word.lower()])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa79833",
   "metadata": {},
   "outputs": [],
   "source": [
    "Î Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î±ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯ Î· ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½, Ï€ÏÎ¹Î½ Ï€ÏÎ¿ÎºÏÏˆÎ¿Ï…Î½ Ï„Î± Ï„ÎµÎ»Î¹ÎºÎ¬ features Ï€Î¿Ï… Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹ Î¿ Classifier.\n",
    "ÎŸÎ¹ ÎµÎ½Î­ÏÎ³ÎµÎ¹ÎµÏ‚ Ï€Î¿Ï… Î³Î¯Î½Î¿Î½Ï„Î±Î¹ ÎµÎ¯Î½Î±Î¹:\n",
    "    1. lookup ÏƒÏ„Î± dictionaries Ï€Î¿Ï… Î±Î½Î±Ï†Î­ÏÎ±Î¼Îµ Î³Î¹Î± Ï„Î·Î½ Î±Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î® Î´Î¹Î±Î³ÏÎ±Ï†Î® Î»Î­Î¾ÎµÏ‰Î½/emojis/emoticons\n",
    "    2. Î”Î¹Î±Î³ÏÎ±Ï†Î® Î¸Î¿ÏÏÎ²Î¿Ï…, ÏŒÏ€Ï‰Ï‚ urls, hashtags, mentions, numbers etc.\n",
    "    3. Tokenization & Î±Ï†Î±Î¯ÏÎµÏƒÎ· stopwords.\n",
    "    4. Part of speech tagging Î³Î¹Î± Î½Î± Ï„Î¿ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ ÏƒÏ„Î·Î½ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Ï„Î¿Ï… Lemmatization.\n",
    "    5. Lemmatization.\n",
    "    6. Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Ï‰Î½ Cleaned/processed data ÏƒÎµ DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].apply(lambda x: lookup_dict(x,emoticons))\n",
    "\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,contraction_dict1))\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,contraction_dict2))\n",
    "\n",
    "data['OriginalTweet'] = data['OriginalTweet'].apply(lambda x: ''.join(''.join(s)[:2] for _, s in itertools.groupby(x)))\n",
    "\n",
    "\n",
    "#to lower case\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.lower()\n",
    "\n",
    "#remove numbers\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('[0-9]', '', regex=True)\n",
    "\n",
    "#remove mentions\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('@([a-zA-Z0-9_]{1,50})', '', regex=True)\n",
    "\n",
    "#remove hashtags\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('#', '', regex=True)\n",
    "\n",
    "#remove urls\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('http\\S+', '', regex=True)\n",
    "\n",
    "# # #remove all remaining bad chars\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace('[^\\\\w\\\\s]', '', regex=True)\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"Ã‚\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"Ã¢\", \"a\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"nz\", \"\", regex=True)\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"_\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"   \", \" \", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"  \", \" \", regex=True)\n",
    "\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"coronavirÂ¼\", \"coronavirus\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"pmmodi\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"amp\", \"and\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"btc\", \"bitcoin\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"hand\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].fillna(0)\n",
    "\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "\n",
    "#Tokenize the tweets\n",
    "tokenized_tweets = data[\"OriginalTweet\"].apply(lambda x: x.split())\n",
    "\n",
    "#remove stopword(for example and,to at etc)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [word for word in x if not word in stop_words])\n",
    "\n",
    "#Stemming the words\n",
    "# stemmer = PorterStemmer()#language='english'\n",
    "# tokenized_tweets= tokenized_tweets.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "\n",
    "def get_pos( word ):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = nltk.Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "\n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "#get the lemma\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [lemmatizer.lemmatize(i,get_pos( i )) for i in x])\n",
    "\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [word for word in x if len(word)>2 or word=='go'])\n",
    "\n",
    "\n",
    "#Joining the tokenized tweets\n",
    "for i in range(len(tokenized_tweets)):\n",
    "    tokenized_tweets[i] = ' '.join(tokenized_tweets[i])\n",
    "data[\"OriginalTweet\"] = tokenized_tweets\n",
    "\n",
    "\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,myOwnStopWords))\n",
    "all_words = []\n",
    "for line in list(data['OriginalTweet']):\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "\n",
    "print(Counter(all_words).most_common(10))\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"  \", \" \", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\" +\", \" \", regex=True)\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "\n",
    "Y = data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Î‘Ï†Î¿Ï Ï„Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÏ„Î®ÎºÎ±Î¼Îµ, Î±ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯ Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Ï„Î¿Ï… feature selection & feature extraction.\n",
    "Î˜Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î· Î³Î½Ï‰ÏƒÏ„Î® TF-IDF Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î±, Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Î½Î± ÎºÎ¬Î½ÎµÎ¹ Î¼Îµ frequency Ï„Ï‰Î½ Î»Î­Î¾ÎµÏ‰Î½/tokens ÏƒÏ„Î¿ Dataset Î³Î¹Î± Î½Î± Î¼Ï€Î¿ÏÎ­ÏƒÎ¿Ï…Î¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ extract ÎºÎ¬Ï€Î¿Î¹Î± Features\n",
    "ÎºÎ±Î¹ Î½Î± ÎºÎ±Ï„Î±Î»Î®Î¾Î¿Ï…Î¼Îµ ÏƒÏ„Î¿ Î³Î½Ï‰ÏƒÏ„ÏŒ Bag of Words. \n",
    "Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ ÎµÏ€Î¯ÏƒÎ·Ï‚ ÎºÎ±Î¹ unigrams & bi-grams. \n",
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ ÎºÎ¬Î½Î¿Ï…Î¼Îµ Î³Î¹Î±Ï„Î¯ Î¼ÎµÏÎ¹ÎºÎ¬ Features Î¸Î± ÎµÎ¯Ï‡Î±Î½ Ï€Î¹Î¿ Ï€Î¿Î»Ï Î½ÏŒÎ·Î¼Î± ÏŒÏƒÎ¿Î½ Î±Ï†Î¿ÏÎ¬ Ï„Î¿ ÏƒÏ…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±, ÎµÎ¬Î½ Î±Ï€Î¿Ï„ÎµÎ»Î»Î¿ÏÎ½Ï„Î±Î¹ Î±Ï€Î¿ 2 tokens Î¼Î±Î¶Î¯ (bigrams).\n",
    "Î“Î¹Î± Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î±:\n",
    "    Î ÏÏŒÏ„Î±ÏƒÎ·: 'I don't like'\n",
    "        unigrams: I, do, not, like\n",
    "        bi-grams: I do, do not, not like\n",
    "ÎŒÏ€Ï‰Ï‚ Î²Î»Î­Ï€Î¿Ï…Î¼Îµ Î· Î»Î­Î¾Î· Like Î±Ï€Î¿ Î¼ÏŒÎ½Î· Ï„Î·Ï‚ Î´ÎµÎ¯Ï‡Î½ÎµÎ¹ ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ Î¸ÎµÏ„Î¹ÎºÏŒ ÏƒÏ…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î± Î­Î½Ï‰ Î¼Îµ Ï„Î·Î½ Î»Î­Î¾Î· Not Î¼Ï€ÏÎ¿ÏƒÏ„Î¬ Î´ÎµÎ¯Ï‡Î½ÎµÎ¹ Î±ÏÎ½Î·Ï„Î¹ÎºÏŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_features=22000,ngram_range=(1,2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(data[\"OriginalTweet\"])\n",
    "print(tfidf.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45277cc0",
   "metadata": {},
   "source": [
    "ÏŒÏ€Ï‰Ï‚ Î²Î»Î­Ï€Î¿Ï…Î¼Îµ Î­Ï‡Î¿Ï…Î¼Îµ Ï€Î¬ÏÎ± Ï€Î¿Î»Î»Î­Ï‚ Î´Î¹Î±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ Î½Î± Î´Î¹Î±Ï‡ÎµÎ¹ÏÎ¹ÏƒÏ„Î¿ÏÎ¼Îµ ÎºÎ±Î¹ Î±Ï…Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ Ï€ÏÏŒÎ²Î»Î·Î¼Î± Ï„ÏŒÏƒÎ¿ Î³Î¹Î± Ï„Î·Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ· Ï„Î¿Ï… Classifier ÏŒÏƒÎ¿ ÎºÎ±Î¹ Î³Î¹Î± Ï„Î¿Î½ Ï‡ÏÏŒÎ½Î¿ train/validation Î±Î»Î»Î¬ ÎºÎ±Î¹ prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(tfidf.todense()[:,np.random.randint(0,tfidf.shape[1],100)]==0,vmin=0,vmax=1,cbar=False).set_title('Sparse Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c1663",
   "metadata": {},
   "source": [
    "Î‘Ï€Î¿ Ï„Î¿ Ï€Î±ÏÎ±Ï€Î¬Î½Ï‰ plot Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Î´Î¿ÏÎ¼Îµ Î¿Ï„Î¹ Ï„Î± Feature Î¼Î±Ï‚ ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï Î±ÏÎµÎ¬ Î¼ÎµÏ„Î±Î¾Ï Ï„Î¿Ï…Ï‚. ÎŸ classifier Ï€Î¹Î¸Î±Î½ÏÏ‚ Î½Î± Î­Ï‡ÎµÎ¹ Ï€ÏÏŒÎ²Î»Î·Î¼Î± ÏƒÎµ Î±Ï…Ï„ÏŒ ÎºÎ±Î¸ÏÏ‚ Î´ÎµÎ½ Î¸Î± Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎºÎ¬Î½ÎµÎ¹ Ï„Î¿ Î²Î­Î»Ï„Î¹ÏƒÏ„Î¿ heneralization. Î•Ï€Î¯ÏƒÎ·Ï‚ Î¸Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î¼ÎµÎ¹Ï‰Î¸Î¿ÏÎ½, ÏÏƒÏ„Îµ Î½Î± Î¼ÎµÎ¹ÏÏƒÎ¿Ï…Î¼Îµ ÎºÎ±Î¹ Ï„Î¹Ï‚ Î´Î¹Î±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚. Î“Î¹Î± Î½Î± Ï„Î¿ Ï€ÎµÏ„ÏÏ‡Î¿Ï…Î¼Îµ Î±Ï…Ï„ÏŒ Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î·Î½ Chi-square ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ® Î¼Î­Î¸Î¿Î´Î¿, Î· Î¿Ï€Î¿Î¯Î± Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Ï…Ï€ÏŒÏˆÎ¹Î½ Ï„Î·Ï‚ Ï„Î·Î½ Î±Î½ÎµÎ¾Î±ÏÏ„Î·ÏƒÎ¯Î± Ï„Ï‰Î½ Ï€Î±ÏÎ±Ï„Î·ÏÎ®ÏƒÎµÏ‰Î½. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = tfidf_vectorizer.get_feature_names_out()\n",
    "p_value_limit = 0.95\n",
    "#this feature selection is ranking features with respect to their usefulness and is not used to make statements about statistical dependence or independence of variables.\n",
    "features = pd.DataFrame()\n",
    "for cat in np.unique(data[\"Sentiment\"]):\n",
    "    chi2, p = feature_selection.chi2(tfidf,data[\"Sentiment\"]==cat)#chi2(tfidf,data[\"Sentiment\"]==cat)\n",
    "    features = features.append(pd.DataFrame({\"feature\":X_names,\"score\":1-p,\"Y\":cat}))\n",
    "    features = features.sort_values([\"Y\",\"score\"],ascending=[True,False])\n",
    "\n",
    "    features = features[features['score']>p_value_limit]\n",
    "\n",
    "X_scores = features[\"score\"].unique().tolist()\n",
    "X_names = features[\"feature\"].unique().tolist()\n",
    "print(X_names)\n",
    "\n",
    "\n",
    "for cat in np.unique(data[\"Sentiment\"]):\n",
    "    print(\"# {}:\".format(cat))\n",
    "    print(\" . selected features:\", len(features[features[\"Y\"]==cat]))\n",
    "    print(\" . top features:\",\",\".join(features[features[\"Y\"]==cat][\"feature\"].values[:20]))\n",
    "    # print(\" . top features scores:\",\",\".join(str(features[features[\"Y\"]==cat][\"score\"].values[:10])))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names,ngram_range=(1,2))\n",
    "tfidf_new = tfidf_vectorizer.fit_transform(data[\"OriginalTweet\"])\n",
    "joblib.dump(tfidf_vectorizer,'vectorizer.sav') #save the BoW model for future work\n",
    "print(tfidf_new.toarray().shape)\n",
    "dic_vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "sns.heatmap(tfidf_new.todense()[:,np.random.randint(0,tfidf_new.shape[1],100)]==0,vmin=0,vmax=1,cbar=False).set_title('Sparse Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf463832",
   "metadata": {},
   "source": [
    "ÎœÏ€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Ï€Î±ÏÎ±Ï„Î·ÏÎ®ÏƒÎ¿Ï…Î¼Îµ Î¿Ï„Î¹ Ï„Î± Features Î­Î³Î¹Î½Î±Î½ Î»Î¹Î³ÏŒÏ„ÎµÏÎ¿ Î±ÏÎµÎ¬.\n",
    "ÎˆÏ„ÏƒÎ¹ ÎµÎ¯Î¼Î±ÏƒÏ„Îµ Î­Ï„Î¿Î¹Î¼Î¿Î¹ Î³Î¹Î± Ï„Î·Î½ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Ï„Î¿Ï… train/validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ebd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = model_selection.train_test_split(tfidf_new, Y, test_size=0.25,shuffle=True,random_state=0)\n",
    "\n",
    "clf2 = naive_bayes.ComplementNB(alpha=1.2).fit(X_train2,y_train2)\n",
    "y_pred = clf2.predict(X_test2)\n",
    "predicted_prob2 = clf2.predict_proba(X_test2)\n",
    "m_confusion_test = metrics.confusion_matrix(y_test2, clf2.predict(X_test2))\n",
    "print(\"NB\")\n",
    "print(pd.DataFrame(data = m_confusion_test , columns=['Predicted -1', 'Predicted 0','Predicted 1'],index=['Actual -1','Actual 0','Actual 1']))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test2, y_pred))\n",
    "print(metrics.classification_report(y_pred,y_test2))\n",
    "\n",
    "clf1 = svm.SVC(kernel='rbf',C=1000,gamma=0.01,probability=True).fit(X_train2,y_train2)\n",
    "y_pred = clf1.predict(X_test2)\n",
    "predicted_prob1 = clf1.predict_proba(X_test2)\n",
    "m_confusion_test = metrics.confusion_matrix(y_test2, clf1.predict(X_test2))\n",
    "print(\"SVM RBF\")\n",
    "print(pd.DataFrame(data = m_confusion_test , columns=['Predicted -1', 'Predicted 0','Predicted 1'],index=['Predicted -1', 'Predicted 0','Predicted 1']))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test2, y_pred))\n",
    "print(metrics.classification_report(y_pred,y_test2))\n",
    "\n",
    "for clf, label in zip([clf1, clf2],\n",
    "                      ['SVM',\n",
    "                       'Naive Bayes'\n",
    "                       ]):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X_train2, y_train2,\n",
    "                                             cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.3f (+/- %0.3f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ae5d8",
   "metadata": {},
   "source": [
    "Î— ÎµÏ€Î¹Î»Î¿Î³Î® Ï„Ï‰Î½ Hyper-parameters Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ ÎµÏ€Î¹Î»Î­Ï‡Î¸Î·ÎºÎµ Î¼ÎµÏ„Î¬ Î±Ï€Î¿ Ï„Î·Î½ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Ï„Î¿Ï… GridSearchCV Ï„Î¿ Î¿Ï€Î¿Î¯Î¿ ÎºÎ¬Î½ÎµÎ¹ Ï„Î±Ï…Ï„ÏŒÏ‡ÏÎ¿Î½Î± ÎºÎ±Î¹ Cross-Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3446b2f5",
   "metadata": {},
   "source": [
    "# # SVM metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_test2)\n",
    "y_test_array = pd.get_dummies(y_test2, drop_first=False).values\n",
    "\n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test2, y_pred)\n",
    "fscore = metrics.f1_score(y_test2, y_pred, average='macro')\n",
    "auc = metrics.roc_auc_score(y_test2, predicted_prob1,\n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"macro F1:\",  round(fscore,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "\n",
    "\n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test2, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues,\n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Predicted\", ylabel=\"Actual\", xticklabels=classes,\n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],\n",
    "                                             predicted_prob1[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3,\n",
    "               label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                                  metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05],\n",
    "          xlabel='False Positive Rate',\n",
    "          ylabel=\"True Positive Rate (Recall)\",\n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "        y_test_array[:,i], predicted_prob1[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3,\n",
    "               label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                                  metrics.auc(recall, precision))\n",
    "               )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',\n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555d8c4",
   "metadata": {},
   "source": [
    "# # Naive Bayes metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cafed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_test2)\n",
    "y_test_array = pd.get_dummies(y_test2, drop_first=False).values\n",
    "\n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(, y_pred)\n",
    "fscore = metrics.f1_score(y_test2, y_pred, average='macro')\n",
    "auc = metrics.roc_auc_score(y_test2, predicted_prob2,\n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"macro F1:\",  round(fscore,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "\n",
    "\n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test2, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues,\n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Predicted\", ylabel=\"Actual\", xticklabels=classes,\n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],\n",
    "                                             predicted_prob2[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3,\n",
    "               label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                                  metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05],\n",
    "          xlabel='False Positive Rate',\n",
    "          ylabel=\"True Positive Rate (Recall)\",\n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "        y_test_array[:,i], predicted_prob2[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3,\n",
    "               label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                                  metrics.auc(recall, precision))\n",
    "               )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',\n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a23655",
   "metadata": {},
   "source": [
    "Î‘Ï€Î¿ Ï„Î± Ï€Î±ÏÎ±Ï€Î¬Î½Ï‰ Plots Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Ï€Î±ÏÎ±Ï„Î·ÏÎ®ÏƒÎ¿Ï…Î¼Îµ Î¿Ï„Î¹ Î¿ SVM Î¼Îµ RBF kernel Î¼Ï€Î¿ÏÎµÎ¯ ÎºÎ±Î»ÏÏ„ÎµÏÎ± Î½Î± ÎºÎ¬Î½ÎµÎ¹ generalize Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Î±Ï‚."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0398f9c8",
   "metadata": {},
   "source": [
    "Î‘Ï‚ Î±Ï€Î¿Î¸Î·ÎºÎµÏÏƒÎ¿Ï…Î¼Îµ Ï„Î± trained models Î³Î¹Î± Î¼ÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenameSVCLinear = 'nb.sav'\n",
    "joblib.dump(clf2, filenameSVCLinear)\n",
    "filenameSVCLinearCV = 'SVC.sav'\n",
    "joblib.dump(clf1, filenameSVCLinearCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cfa960",
   "metadata": {},
   "source": [
    "# Î”Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Test ÏƒÎµ unknown Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5dbe98",
   "metadata": {},
   "source": [
    "Î˜Î± Î±ÎºÎ¿Î»Î¿Ï…Î¸Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î·Î½ Î¯Î´Î¹Î± Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± processing Ï€Î¿Ï… Î±ÎºÎ¿Î»Î¿Ï…Î¸Î®ÏƒÎ±Î¼Îµ Î³Î¹Î± Ï„Î¿ training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test csv file\n",
    "data = pd.read_csv(r\"Corona_NLP_test.csv\", encoding='ansi')\n",
    "data[\"Sentiment\"] = data[\"Sentiment\"].replace('Extremely Negative', 'Negative', regex=True)\n",
    "\n",
    "data[\"Sentiment\"] = data[\"Sentiment\"].replace('Extremely Positive', 'Positive', regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Negative', -1, regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Positive', 1, regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Neutral', 0, regex=True)\n",
    "#to lower case\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].apply(lambda x: lookup_dict(x,emoticons))\n",
    "\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,contraction_dict1))\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,contraction_dict2))\n",
    "\n",
    "data['OriginalTweet'] = data['OriginalTweet'].apply(lambda x: ''.join(''.join(s)[:2] for _, s in itertools.groupby(x)))\n",
    "\n",
    "\n",
    "#to lower case\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.lower()\n",
    "\n",
    "#remove numbers\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('[0-9]', '', regex=True)\n",
    "\n",
    "#remove mentions\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('@([a-zA-Z0-9_]{1,50})', '', regex=True)\n",
    "\n",
    "#remove hashtags\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('#', '', regex=True)\n",
    "\n",
    "#remove urls\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('http\\S+', '', regex=True)\n",
    "\n",
    "# # #remove all remaining bad chars\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace('[^\\\\w\\\\s]', '', regex=True)\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"Ã‚\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"Ã¢\", \"a\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"nz\", \"\", regex=True)\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"_\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"   \", \" \", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"  \", \" \", regex=True)\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"coronavirÂ¼\", \"coronavirus\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"pmmodi\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"amp\", \"and\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].fillna(0)\n",
    "\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "#Tokenize the tweets\n",
    "tokenized_tweets = data[\"OriginalTweet\"].apply(lambda x: x.split())\n",
    "\n",
    "#remove stopword(for example and,to at etc)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [word for word in x if not word in stop_words])\n",
    "\n",
    "# stemmer = PorterStemmer()#language='english'\n",
    "# tokenized_tweets= tokenized_tweets.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [lemmatizer.lemmatize(i,get_pos( i )) for i in x])\n",
    "\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [word for word in x if len(word)>2 or word=='go'])\n",
    "# tokenized_tweets= tokenized_tweets.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "\n",
    "#Joining the tokenized tweets\n",
    "for i in range(len(tokenized_tweets)):\n",
    "    tokenized_tweets[i] = ' '.join(tokenized_tweets[i])\n",
    "data[\"OriginalTweet\"] = tokenized_tweets\n",
    "\n",
    "\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,myOwnStopWords))\n",
    "all_words = []\n",
    "for line in list(data['OriginalTweet']):\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"  \", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5b733",
   "metadata": {},
   "source": [
    "Î“Î¹Î± Ï„Î¿ Feature extraction Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ Î¯Î´Î¹Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ BoW (TF-IDF) Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ±Î¼Îµ Ï€Î±ÏÎ±Ï€Î¬Î½Ï‰ ÏƒÏ„Î¿ training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf= tfidf_vectorizer.transform(data[\"OriginalTweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56932736",
   "metadata": {},
   "source": [
    "Î‘Ï‚ Ï€Î¬Î¼Îµ Î½Î± Î´Î¿ÏÎ¼Îµ Ï„Î± predictions kai Ï„Î¿Ï…Ï‚ Ï‡ÏÏŒÎ½Î¿Ï…Ï‚ Ï€Î¿Ï… Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î­Î½Î±Ï‚ Î±Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Ï‚ Î³Î¹Î± Î½Î± ÎºÎ¬Î½ÎµÎ¹ Ï„Î¿ predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d08f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "y_pred= clf1.predict(tfidf)\n",
    "duration = time.time() - start\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\",metrics.accuracy_score(data[\"Sentiment\"], y_pred))\n",
    "print(metrics.classification_report(y_pred,data[\"Sentiment\"]))\n",
    "print(\"Duration: \"+str(duration))\n",
    "\n",
    "start2 = time.time()\n",
    "y_pred2= clf2.predict(tfidf)\n",
    "duration2 = time.time() - start2\n",
    "print(\"NB\")\n",
    "print(\"Accuracy:\",metrics.accuracy_score(data[\"Sentiment\"], y_pred2))\n",
    "print(metrics.classification_report(y_pred2,data[\"Sentiment\"]))\n",
    "print(\"Duration: \"+str(duration2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
