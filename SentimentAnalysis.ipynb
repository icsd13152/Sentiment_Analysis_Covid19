{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08729a88",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Covid-19 Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ac6bb",
   "metadata": {},
   "source": [
    "# # Abstract\n",
    "Κατά την διάρκεια της πανδημίας, πολλοί χρήστες του Twitter Δημοσιεύουν διάφορα Posts που αποτελούν την γνώμη του σχετικά με τον Covid-19. Θα ήταν καλό να γνωρίζουμε το συναίσθημα που προκύπτει από το κάθε post, προκειμένου να δούμε κατά πόσο έχει επηρεαστεί η ψυχολογία του εκάστοτε χρήστη."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca1665",
   "metadata": {},
   "source": [
    "Τα δεδομένα που χρησιμοποιήθηκαν κατα τις training/validation διαδικασίες μπορεί κανείς να τα βρεί στο Kaggle στο παρακάτω URL:\n",
    "https://www.kaggle.com/gauravsahani/covid-19-sentiment-analysis-using-spacy/data\n",
    "\n",
    "Μπορούμε να κατεβάσουμε τα δεδομένα με την παρακάτω command:\n",
    "\n",
    "$ kaggle kernels output gauravsahani/covid-19-sentiment-analysis-using-spacy -p /path/to/dest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa784d0",
   "metadata": {},
   "source": [
    "# # Data Processing\n",
    "Παρακάτω αφού έχουμε κατεβάσει τα δεδομένα απο το Kaggle, διαβάζουμε το csv file το οποίο αφορά τα δεδομένα του Training.\n",
    "Όπως θα παρατηρήσουμε στο Kaggle τα δεδομένα μας είναι χωρισμένα σε 2 csv αρχεία. Ένα Train και ένα Test.\n",
    "Έτσι λοιπόν αφού κάνουμε import τις απαραίτητες βιβλιοθήκες που θα χρειαστούμε μπορούμε να διαβάσουμε τα csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2cfb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserName  ScreenName   Location     TweetAt  \\\n",
      "0      3799       48751     London  16-03-2020   \n",
      "1      3800       48752         UK  16-03-2020   \n",
      "2      3801       48753  Vagabonds  16-03-2020   \n",
      "3      3802       48754        NaN  16-03-2020   \n",
      "4      3803       48755        NaN  16-03-2020   \n",
      "\n",
      "                                       OriginalTweet           Sentiment  \n",
      "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
      "1  advice Talk to your neighbours family to excha...            Positive  \n",
      "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
      "3  My food stock is not the only one which is emp...            Positive  \n",
      "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  \n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\n",
    "import nltk\n",
    "from nltk.corpus import  wordnet\n",
    "# nltk.download('wordnet')\n",
    "from nltk import FreqDist, PorterStemmer, SnowballStemmer, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn import feature_selection\n",
    "\n",
    "data = pd.read_csv(r\"Corona_NLP_train.csv\", encoding='ansi')\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e8d259",
   "metadata": {},
   "source": [
    "Αφού διαβάσαμε τα δεδομένα απο το CSV αρχείο και τα φέραμε σε ένα DataFrame ακολουθεί η επεξεργασια αυτών και η διερεύνηση για το τι δεδομένα έχουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b8005d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"Sentiment\"] = data[\"Sentiment\"].replace('Extremely Negative', 'Negative', regex=True)\n",
    "\n",
    "data[\"Sentiment\"] = data[\"Sentiment\"].replace('Extremely Positive', 'Positive', regex=True)\n",
    "\n",
    "#transform Sentiment to number\n",
    "#negative=0\n",
    "#postive=1\n",
    "#neutral=2\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Negative', -1, regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Positive', 1, regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Neutral', 0, regex=True)\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(data[\"Sentiment\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79db5f6",
   "metadata": {},
   "source": [
    "Όπως μπορούμε να δούμε για την Class = Negative = -1 έχουμε περίπου 15.500 samples. Ομοίως για τη Class = Positive = 1 έχουμε περίπου 17.500 samples. Τέλος για την Class = Neutral = 0 έχουμε περίπου 7.500 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0b246",
   "metadata": {},
   "source": [
    "## Ακολουθεί η διαδικασία του Processing:\n",
    "Μετά απο πολύ ψάξιμο εντός των δεδομένων παρατήρησα οτι έχουμε κάποιες περίεργες λέξεις και κάποιες λέξεις με απόστροφο. \n",
    "Επίσης είναι λογικό σε ένα tweet να βρει κανείς και emojis/emoticons. Όλα αυτά αποθηκεύτηκαν σε κάποια dictionariesμε σκοπό να κάνουμε lookups\n",
    "για να μπρούμε να τα αντικαθιστούμε ή να τα διαγράφουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae492c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#short words/apostrophe lookup\n",
    "contraction_dict1 = {\"Â\":\"\",\"’\":\"'\",\"Ã\":\"\"}\n",
    "contraction_dict2 = {\"Â\":\"\",\"’\":\"'\",\"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\"don't\": \"do not\",\"Don't\":\"Do not\",\n",
    "                     \"I'll\":\"I will\",\"Didn't\":\"Did not\",\"hasn't\":\"has not\",\"NYC\":\"New York City\",\"16MAR20\":\"\",\n",
    "                     \"I'd\":\"I would\",\"I've\":\"I have\",\"you're\":\"you are\",\"I'm\":\"I am\",\"it's\":\"it is\",\n",
    "                     \"#NZ\":\"\",\"they'll\":\"they will\",\"they're\":\"they are\",\"can't\":\"can not\",\"Y'all\":\"You All\",\n",
    "                     \"I m\":\"I am\",\"can't\":\"can not\",\"don t\":\"do not\",\"I ve\":\"I have\",\"we're\":\"we are\",\n",
    "                     \"LOL\":\"lough out loud\",\"lol\":\"lough out loud\",\"FYI\":\"For your information\",\"OFC\":\"Of Course\",\"ofc\":\"Of Course\",\n",
    "                     \"#coronavirÃ¼s\":\"coronavirus\",\"pls\":\"please\",\"#stayhomesavelives\":\"stay home save lives\",\n",
    "                     \"hasn't\": 'has not',\"haven't\": 'have not',\"he'd\": 'he had / he would',\"he'd've\": 'he would have',\n",
    "                     \"he'll\": 'he shall / he will',\"he'll've\": 'he shall have / he will have',\n",
    "                     \"he's\": 'he has / he is',\"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will',\n",
    "                     \"how's\": 'how has / how is', \"i'd\": 'I had / I would',\"i'd've\": 'I would have',  \"i'll\": 'I shall / I will',\n",
    "                     \"i'll've\": 'I shall have / I will have',\"i'm\": 'I am', \"i've\": 'I have', \"isn't\": 'is not', \"it'd\": 'it had / it would',\n",
    "                     \"it'd've\": 'it would have', \"it'll\": 'it shall / it will',\n",
    "                     \"it'll've\": 'it shall have / it will have',\"it's\": 'it has / it is', \"let's\": 'let us',\n",
    "                     \"ma'am\": 'madam', \"mayn't\": 'may not',\n",
    "                     \"might've\": 'might have', \"mightn't\": 'might not',\n",
    "                     \"mightn't've\": 'might not have', \"must've\": 'must have',\"mustn't\": 'must not',\n",
    "                     \"mustn't've\": 'must not have', \"needn't\": 'need not',\n",
    "                     \"needn't've\": 'need not have', \"o'clock\": 'of the clock',\n",
    "                     \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have',\n",
    "                     \"shan't\": 'shall not', \"sha'n't\": 'shall not',\n",
    "                     \"shan't've\": 'shall not have', \"she'd\": 'she had / she would',\n",
    "                     \"she'd've\": 'she would have', \"she'll\": 'she shall / she will',\n",
    "                     \"she'll've\": 'she shall have / she will have',\n",
    "                     \"she's\": 'she has / she is', \"should've\": 'should have',\n",
    "                     \"shouldn't\": 'should not',\"shouldn't've\": 'should not have',\n",
    "                     \"so've\": 'so have', \"so's\": 'so as / so is',\n",
    "                     \"that'd\": 'that would / that had',\"that'd've\": 'that would have',\n",
    "                     \"that's\": 'that has / that is', \"there'd\": 'there had / there would',\n",
    "                     \"there'd've\": 'there would have', \"there's\": 'there has / there is',\n",
    "                     \"they'd\": 'they had / they would',  \"they'd've\": 'they would have',\n",
    "                     \"they'll\": 'they shall / they will', \"they'll've\": 'they shall have / they will have',\n",
    "                     \"they're\": 'they are',  \"they've\": 'they have',\n",
    "                     \"to've\": 'to have', \"wasn't\": 'was not',\n",
    "                     \"we'd\": 'we had / we would',  \"we'd've\": 'we would have',\n",
    "                     \"we'll\": 'we will', \"we'll've\": 'we will have',\n",
    "                     \"we're\": 'we are', \"we've\": 'we have',\n",
    "                     \"weren't\": 'were not', \"what'll\": 'what shall / what will',\n",
    "                     \"what'll've\": 'what shall have / what will have',\n",
    "                     \"what're\": 'what are', \"what's\": 'what has / what is',\n",
    "                     \"what've\": 'what have',\"when's\": 'when has / when is',\n",
    "                     \"when've\": 'when have', \"where'd\": 'where did',\n",
    "                     \"where's\": 'where has / where is',\n",
    "                     \"where've\": 'where have', \"who'll\": 'who shall / who will',\n",
    "                     \"who'll've\": 'who shall have / who will have',\n",
    "                     \"who's\": 'who has / who is', \"who've\": 'who have',\n",
    "                     \"why's\": 'why has / why is', \"why've\": 'why have',\n",
    "                     \"will've\": 'will have', \"won't\": 'will not',\"won't've\": 'will not have',\n",
    "                     \"would've\": 'would have',\"wouldn't\": 'would not',\"wouldn't've\": 'would not have',\n",
    "                     \"y'all\": 'you all', \"y'all'd\": 'you all would',\n",
    "                     \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are',\n",
    "                     \"y'all've\": 'you all have', \"you'd\": 'you had / you would',\n",
    "                     \"you'd've\": 'you would have',\"&amp\":\"and\",\"btc\":\"bitcoin\",\"irs\":\"\",\"spx\":\"\",\"📍\":\"\",\"✅\":\"\"\n",
    "                     }\n",
    "\n",
    "emoticons={':)': 'happy', ':‑)': 'happy',\n",
    " ':-]': 'happy', ':-3': 'happy',\n",
    " ':->': 'happy', '8-)': 'happy',\n",
    " ':-}': 'happy', ':o)': 'happy',\n",
    " ':c)': 'happy', ':^)': 'happy',\n",
    " '=]': 'happy', '=)': 'happy',\n",
    " '<3': 'happy', ':-(': 'sad',\n",
    " ':(': 'sad', ':c': 'sad',\n",
    " ':<': 'sad', ':[': 'sad',\n",
    " '>:[': 'sad', ':{': 'sad',\n",
    " '>:(': 'sad', ':-c': 'sad',\n",
    " ':-< ': 'sad', ':-[': 'sad',\n",
    " ':-||': 'sad',\n",
    "  '😢':'sad'         }\n",
    "\n",
    "myOwnStopWords={'price':\"\",\n",
    "               'store':\"\",\n",
    "               'supermarket':\"\",\n",
    "               'food':\"\",\n",
    "               'grocery':\"\",\n",
    "               'people':\"\",\n",
    "               'go':\"\",\n",
    "               'consumer':\"\",\n",
    "                'usdjpy':\"\", 'gbpusd':\"\", 'usdcnh':\"\", 'xauusd':\"\", 'wti':\"\", 'spx':\"\",'iave':\"\",\"aiave\":\"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87682d9d",
   "metadata": {},
   "source": [
    "Σκοπός δεν είναι να σβήσουμε τα emojis/emoticos αλλά να τα κάνουμε replace με την λέξη που δείχνουν. Με αυτόν τον τρόπο θα μπροούμε να κρατήσουμε το συναίσθημα που προβάλλουν. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9cacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_dict(text, dictionary):\n",
    "    if isinstance(text, float) == False and text is not None:\n",
    "        for word in text.split():\n",
    "            if word.lower() in dictionary:\n",
    "                if word.lower() in text.split():\n",
    "                    text = text.replace(word, dictionary[word.lower()])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa79833",
   "metadata": {},
   "outputs": [],
   "source": [
    "Παρακάτω ακολουθεί η επεξεργασία των δεδομένων, πριν προκύψουν τα τελικά features που θα χρησιμοποιήσει ο Classifier.\n",
    "Οι ενέργειες που γίνονται είναι:\n",
    "    1. lookup στα dictionaries που αναφέραμε για την αντικατάσταση ή διαγραφή λέξεων/emojis/emoticons\n",
    "    2. Διαγραφή θορύβου, όπως urls, hashtags, mentions, numbers etc.\n",
    "    3. Tokenization & αφαίρεση stopwords.\n",
    "    4. Part of speech tagging για να το χρησιμοποιήσουμε στην διαδικασία του Lemmatization.\n",
    "    5. Lemmatization.\n",
    "    6. Αποθήκευση των Cleaned/processed data σε DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].apply(lambda x: lookup_dict(x,emoticons))\n",
    "\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,contraction_dict1))\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,contraction_dict2))\n",
    "\n",
    "data['OriginalTweet'] = data['OriginalTweet'].apply(lambda x: ''.join(''.join(s)[:2] for _, s in itertools.groupby(x)))\n",
    "\n",
    "\n",
    "#to lower case\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.lower()\n",
    "\n",
    "#remove numbers\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('[0-9]', '', regex=True)\n",
    "\n",
    "#remove mentions\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('@([a-zA-Z0-9_]{1,50})', '', regex=True)\n",
    "\n",
    "#remove hashtags\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('#', '', regex=True)\n",
    "\n",
    "#remove urls\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('http\\S+', '', regex=True)\n",
    "\n",
    "# # #remove all remaining bad chars\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace('[^\\\\w\\\\s]', '', regex=True)\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"Â\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"â\", \"a\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"nz\", \"\", regex=True)\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"_\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"   \", \" \", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"  \", \" \", regex=True)\n",
    "\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"coronavir¼\", \"coronavirus\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"pmmodi\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"amp\", \"and\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"btc\", \"bitcoin\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"hand\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].fillna(0)\n",
    "\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "\n",
    "#Tokenize the tweets\n",
    "tokenized_tweets = data[\"OriginalTweet\"].apply(lambda x: x.split())\n",
    "\n",
    "#remove stopword(for example and,to at etc)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [word for word in x if not word in stop_words])\n",
    "\n",
    "#Stemming the words\n",
    "# stemmer = PorterStemmer()#language='english'\n",
    "# tokenized_tweets= tokenized_tweets.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "\n",
    "def get_pos( word ):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = nltk.Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "\n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "#get the lemma\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [lemmatizer.lemmatize(i,get_pos( i )) for i in x])\n",
    "\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [word for word in x if len(word)>2 or word=='go'])\n",
    "\n",
    "\n",
    "#Joining the tokenized tweets\n",
    "for i in range(len(tokenized_tweets)):\n",
    "    tokenized_tweets[i] = ' '.join(tokenized_tweets[i])\n",
    "data[\"OriginalTweet\"] = tokenized_tweets\n",
    "\n",
    "\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,myOwnStopWords))\n",
    "all_words = []\n",
    "for line in list(data['OriginalTweet']):\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "\n",
    "print(Counter(all_words).most_common(10))\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"  \", \" \", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\" +\", \" \", regex=True)\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "\n",
    "Y = data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Αφού τα επεξεργαστήκαμε, ακολουθεί η διαδικασία του feature selection & feature extraction.\n",
    "Θα χρησιμοποιήσουμε τη γνωστή TF-IDF διαδικασία, που έχει να κάνει με frequency των λέξεων/tokens στο Dataset για να μπορέσουμε να κάνουμε extract κάποια Features\n",
    "και να καταλήξουμε στο γνωστό Bag of Words. \n",
    "Χρησιμοποιούμε επίσης και unigrams & bi-grams. \n",
    "Αυτό το κάνουμε γιατί μερικά Features θα είχαν πιο πολύ νόημα όσον αφορά το συναίσθημα, εάν αποτελλούνται απο 2 tokens μαζί (bigrams).\n",
    "Για παράδειγμα:\n",
    "    Πρόταση: 'I don't like'\n",
    "        unigrams: I, do, not, like\n",
    "        bi-grams: I do, do not, not like\n",
    "Όπως βλέπουμε η λέξη Like απο μόνη της δείχνει συνήθως θετικό συναίσθημα ένω με την λέξη Not μπροστά δείχνει αρνητικό."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_features=22000,ngram_range=(1,2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(data[\"OriginalTweet\"])\n",
    "print(tfidf.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45277cc0",
   "metadata": {},
   "source": [
    "όπως βλέπουμε έχουμε πάρα πολλές διαστάσεις να διαχειριστούμε και αυτό είναι πρόβλημα τόσο για την απόδοση του Classifier όσο και για τον χρόνο train/validation αλλά και prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(tfidf.todense()[:,np.random.randint(0,tfidf.shape[1],100)]==0,vmin=0,vmax=1,cbar=False).set_title('Sparse Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c1663",
   "metadata": {},
   "source": [
    "Απο το παραπάνω plot μπορούμε να δούμε οτι τα Feature μας είναι πολύ αρεά μεταξύ τους. Ο classifier πιθανώς να έχει πρόβλημα σε αυτό καθώς δεν θα μπορεί να κάνει το βέλτιστο heneralization. Επίσης θα πρέπει να μειωθούν, ώστε να μειώσουμε και τις διαστάσεις. Για να το πετύχουμε αυτό θα χρησιμοποιήσουμε την Chi-square στατιστική μέθοδο, η οποία λαμβάνει υπόψιν της την ανεξαρτησία των παρατηρήσεων. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = tfidf_vectorizer.get_feature_names_out()\n",
    "p_value_limit = 0.95\n",
    "#this feature selection is ranking features with respect to their usefulness and is not used to make statements about statistical dependence or independence of variables.\n",
    "features = pd.DataFrame()\n",
    "for cat in np.unique(data[\"Sentiment\"]):\n",
    "    chi2, p = feature_selection.chi2(tfidf,data[\"Sentiment\"]==cat)#chi2(tfidf,data[\"Sentiment\"]==cat)\n",
    "    features = features.append(pd.DataFrame({\"feature\":X_names,\"score\":1-p,\"Y\":cat}))\n",
    "    features = features.sort_values([\"Y\",\"score\"],ascending=[True,False])\n",
    "\n",
    "    features = features[features['score']>p_value_limit]\n",
    "\n",
    "X_scores = features[\"score\"].unique().tolist()\n",
    "X_names = features[\"feature\"].unique().tolist()\n",
    "print(X_names)\n",
    "\n",
    "\n",
    "for cat in np.unique(data[\"Sentiment\"]):\n",
    "    print(\"# {}:\".format(cat))\n",
    "    print(\" . selected features:\", len(features[features[\"Y\"]==cat]))\n",
    "    print(\" . top features:\",\",\".join(features[features[\"Y\"]==cat][\"feature\"].values[:20]))\n",
    "    # print(\" . top features scores:\",\",\".join(str(features[features[\"Y\"]==cat][\"score\"].values[:10])))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names,ngram_range=(1,2))\n",
    "tfidf_new = tfidf_vectorizer.fit_transform(data[\"OriginalTweet\"])\n",
    "joblib.dump(tfidf_vectorizer,'vectorizer.sav') #save the BoW model for future work\n",
    "print(tfidf_new.toarray().shape)\n",
    "dic_vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "sns.heatmap(tfidf_new.todense()[:,np.random.randint(0,tfidf_new.shape[1],100)]==0,vmin=0,vmax=1,cbar=False).set_title('Sparse Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf463832",
   "metadata": {},
   "source": [
    "Μπορούμε να παρατηρήσουμε οτι τα Features έγιναν λιγότερο αρεά.\n",
    "Έτσι είμαστε έτοιμοι για την διαδικασία του train/validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ebd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = model_selection.train_test_split(tfidf_new, Y, test_size=0.25,shuffle=True,random_state=0)\n",
    "\n",
    "clf2 = naive_bayes.ComplementNB(alpha=1.2).fit(X_train2,y_train2)\n",
    "y_pred = clf2.predict(X_test2)\n",
    "predicted_prob2 = clf2.predict_proba(X_test2)\n",
    "m_confusion_test = metrics.confusion_matrix(y_test2, clf2.predict(X_test2))\n",
    "print(\"NB\")\n",
    "print(pd.DataFrame(data = m_confusion_test , columns=['Predicted -1', 'Predicted 0','Predicted 1'],index=['Actual -1','Actual 0','Actual 1']))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test2, y_pred))\n",
    "print(metrics.classification_report(y_pred,y_test2))\n",
    "\n",
    "clf1 = svm.SVC(kernel='rbf',C=1000,gamma=0.01,probability=True).fit(X_train2,y_train2)\n",
    "y_pred = clf1.predict(X_test2)\n",
    "predicted_prob1 = clf1.predict_proba(X_test2)\n",
    "m_confusion_test = metrics.confusion_matrix(y_test2, clf1.predict(X_test2))\n",
    "print(\"SVM RBF\")\n",
    "print(pd.DataFrame(data = m_confusion_test , columns=['Predicted -1', 'Predicted 0','Predicted 1'],index=['Predicted -1', 'Predicted 0','Predicted 1']))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test2, y_pred))\n",
    "print(metrics.classification_report(y_pred,y_test2))\n",
    "\n",
    "for clf, label in zip([clf1, clf2],\n",
    "                      ['SVM',\n",
    "                       'Naive Bayes'\n",
    "                       ]):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X_train2, y_train2,\n",
    "                                             cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.3f (+/- %0.3f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ae5d8",
   "metadata": {},
   "source": [
    "Η επιλογή των Hyper-parameters των μοντέλων επιλέχθηκε μετά απο την διαδικασία του GridSearchCV το οποίο κάνει ταυτόχρονα και Cross-Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3446b2f5",
   "metadata": {},
   "source": [
    "# # SVM metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_test2)\n",
    "y_test_array = pd.get_dummies(y_test2, drop_first=False).values\n",
    "\n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test2, y_pred)\n",
    "fscore = metrics.f1_score(y_test2, y_pred, average='macro')\n",
    "auc = metrics.roc_auc_score(y_test2, predicted_prob1,\n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"macro F1:\",  round(fscore,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "\n",
    "\n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test2, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues,\n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Predicted\", ylabel=\"Actual\", xticklabels=classes,\n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],\n",
    "                                             predicted_prob1[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3,\n",
    "               label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                                  metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05],\n",
    "          xlabel='False Positive Rate',\n",
    "          ylabel=\"True Positive Rate (Recall)\",\n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "        y_test_array[:,i], predicted_prob1[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3,\n",
    "               label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                                  metrics.auc(recall, precision))\n",
    "               )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',\n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555d8c4",
   "metadata": {},
   "source": [
    "# # Naive Bayes metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cafed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_test2)\n",
    "y_test_array = pd.get_dummies(y_test2, drop_first=False).values\n",
    "\n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(, y_pred)\n",
    "fscore = metrics.f1_score(y_test2, y_pred, average='macro')\n",
    "auc = metrics.roc_auc_score(y_test2, predicted_prob2,\n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"macro F1:\",  round(fscore,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "\n",
    "\n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test2, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues,\n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Predicted\", ylabel=\"Actual\", xticklabels=classes,\n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],\n",
    "                                             predicted_prob2[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3,\n",
    "               label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                                  metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05],\n",
    "          xlabel='False Positive Rate',\n",
    "          ylabel=\"True Positive Rate (Recall)\",\n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "        y_test_array[:,i], predicted_prob2[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3,\n",
    "               label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                                  metrics.auc(recall, precision))\n",
    "               )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',\n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a23655",
   "metadata": {},
   "source": [
    "Απο τα παραπάνω Plots μπορούμε να παρατηρήσουμε οτι ο SVM με RBF kernel μπορεί καλύτερα να κάνει generalize τα δεδομένα μας."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0398f9c8",
   "metadata": {},
   "source": [
    "Ας αποθηκεύσουμε τα trained models για μελλοντική χρήση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenameSVCLinear = 'nb.sav'\n",
    "joblib.dump(clf2, filenameSVCLinear)\n",
    "filenameSVCLinearCV = 'SVC.sav'\n",
    "joblib.dump(clf1, filenameSVCLinearCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cfa960",
   "metadata": {},
   "source": [
    "# Διαδικασία Test σε unknown Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5dbe98",
   "metadata": {},
   "source": [
    "Θα ακολουθήσουμε την ίδια διαδικασία processing που ακολουθήσαμε για το training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test csv file\n",
    "data = pd.read_csv(r\"Corona_NLP_test.csv\", encoding='ansi')\n",
    "data[\"Sentiment\"] = data[\"Sentiment\"].replace('Extremely Negative', 'Negative', regex=True)\n",
    "\n",
    "data[\"Sentiment\"] = data[\"Sentiment\"].replace('Extremely Positive', 'Positive', regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Negative', -1, regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Positive', 1, regex=True)\n",
    "data[\"Sentiment\"]=data[\"Sentiment\"].replace('Neutral', 0, regex=True)\n",
    "#to lower case\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].apply(lambda x: lookup_dict(x,emoticons))\n",
    "\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,contraction_dict1))\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,contraction_dict2))\n",
    "\n",
    "data['OriginalTweet'] = data['OriginalTweet'].apply(lambda x: ''.join(''.join(s)[:2] for _, s in itertools.groupby(x)))\n",
    "\n",
    "\n",
    "#to lower case\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.lower()\n",
    "\n",
    "#remove numbers\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('[0-9]', '', regex=True)\n",
    "\n",
    "#remove mentions\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('@([a-zA-Z0-9_]{1,50})', '', regex=True)\n",
    "\n",
    "#remove hashtags\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('#', '', regex=True)\n",
    "\n",
    "#remove urls\n",
    "data[\"OriginalTweet\"] = data[\"OriginalTweet\"].replace('http\\S+', '', regex=True)\n",
    "\n",
    "# # #remove all remaining bad chars\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace('[^\\\\w\\\\s]', '', regex=True)\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"Â\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"â\", \"a\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"nz\", \"\", regex=True)\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"_\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"   \", \" \", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"  \", \" \", regex=True)\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"coronavir¼\", \"coronavirus\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"pmmodi\", \"\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"amp\", \"and\", regex=True)\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].fillna(0)\n",
    "\n",
    "data['OriginalTweet']  = data['OriginalTweet'].str.strip()\n",
    "#Tokenize the tweets\n",
    "tokenized_tweets = data[\"OriginalTweet\"].apply(lambda x: x.split())\n",
    "\n",
    "#remove stopword(for example and,to at etc)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [word for word in x if not word in stop_words])\n",
    "\n",
    "# stemmer = PorterStemmer()#language='english'\n",
    "# tokenized_tweets= tokenized_tweets.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [lemmatizer.lemmatize(i,get_pos( i )) for i in x])\n",
    "\n",
    "tokenized_tweets = tokenized_tweets.apply(lambda x: [word for word in x if len(word)>2 or word=='go'])\n",
    "# tokenized_tweets= tokenized_tweets.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "\n",
    "#Joining the tokenized tweets\n",
    "for i in range(len(tokenized_tweets)):\n",
    "    tokenized_tweets[i] = ' '.join(tokenized_tweets[i])\n",
    "data[\"OriginalTweet\"] = tokenized_tweets\n",
    "\n",
    "\n",
    "data['OriginalTweet']=data['OriginalTweet'].apply(lambda x:lookup_dict(x,myOwnStopWords))\n",
    "all_words = []\n",
    "for line in list(data['OriginalTweet']):\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "\n",
    "data[\"OriginalTweet\"]=data[\"OriginalTweet\"].replace(\"  \", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5b733",
   "metadata": {},
   "source": [
    "Για το Feature extraction θα χρησιμοποιήσουμε το ίδιο μοντέλο BoW (TF-IDF) που χρησιμοποιήσαμε παραπάνω στο training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf= tfidf_vectorizer.transform(data[\"OriginalTweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56932736",
   "metadata": {},
   "source": [
    "Ας πάμε να δούμε τα predictions kai τους χρόνους που χρειάζεται ένας αλγόριθμος για να κάνει το predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d08f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "y_pred= clf1.predict(tfidf)\n",
    "duration = time.time() - start\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\",metrics.accuracy_score(data[\"Sentiment\"], y_pred))\n",
    "print(metrics.classification_report(y_pred,data[\"Sentiment\"]))\n",
    "print(\"Duration: \"+str(duration))\n",
    "\n",
    "start2 = time.time()\n",
    "y_pred2= clf2.predict(tfidf)\n",
    "duration2 = time.time() - start2\n",
    "print(\"NB\")\n",
    "print(\"Accuracy:\",metrics.accuracy_score(data[\"Sentiment\"], y_pred2))\n",
    "print(metrics.classification_report(y_pred2,data[\"Sentiment\"]))\n",
    "print(\"Duration: \"+str(duration2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
